
%----------------------------------------------------------------------------------------
%	Machine Learning Assignment Template
%----------------------------------------------------------------------------------------

\documentclass[unicode, 11pt, a4paper]{scrartcl}
\newcommand*\student[1]{\newcommand{\thestudent}{{#1}}}

%----------------------------------------------------------------------------------------
%	INSERT HERE YOUR NAME
%----------------------------------------------------------------------------------------

\student{Morales Mariciano Jeferson}

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Use 8-bit encoding
\usepackage[sc]{mathpazo} % serif font for math
\usepackage{caption, subcaption}
\usepackage{hyperref} % links
\usepackage{inconsolata} % monospaced font

\usepackage[english]{babel} % English language hyphenation
\usepackage{amsmath, amsfonts} % Math packages
\usepackage{listings} % Code listings, with syntax highlighting
\usepackage{graphicx} % Required for inserting images
\graphicspath{{Figures/}{./}} % Specifies where to look for included images (trailing slash required)
\usepackage{float} % positioning

%----------------------------------------------------------------------------------------
%	DOCUMENT MARGINS
%----------------------------------------------------------------------------------------

\usepackage{geometry} % For page dimensions and margins
\geometry{
	paper=a4paper, 
	top=2.5cm, % Top margin
	bottom=3cm, % Bottom margin
	left=3cm, % Left margin
	right=3cm, % Right margin
}

%----------------------------------------------------------------------------------------
%	SECTION TITLES
%----------------------------------------------------------------------------------------

\usepackage{sectsty}
\sectionfont{\vspace{6pt}\centering\normalfont\scshape}
\subsectionfont{\normalfont\bfseries} % \subsection{} styling
\subsubsectionfont{\normalfont\itshape} % \subsubsection{} styling
\paragraphfont{\normalfont\scshape} % \paragraph{} styling

%----------------------------------------------------------------------------------------
%	HEADERS AND FOOTERS
%----------------------------------------------------------------------------------------

\usepackage{scrlayer-scrpage} % for footer and header customization
\ofoot*{\pagemark} % Right footer
\ifoot*{\thestudent} % Left footer
\cfoot*{} % Centre footer

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{	
	\normalfont\normalsize
	\textsc{Machine Learning\\%
	Universit\`a della Svizzera italiana}\\
	\vspace{25pt}
	\rule{\linewidth}{0.5pt}\\
	\vspace{20pt}
	{\huge Assignment 1}\\
	\vspace{12pt}
	\rule{\linewidth}{1pt}\\
	\vspace{12pt}
}

\author{\LARGE \thestudent}

\date{\normalsize\today}

% --- CUSTOM MACRO -------------------------------------------------------------------------
\newcommand{\myvec}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\myex}[1]{\begin{equation*}\begin{aligned} #1 \end{aligned}\end{equation*}}
\newcommand{\myFigure}[3]{
    \begin{figure}[htbp]
    \centering
    \caption{#1}
    \label{#2}
    \includegraphics[width=.7\textwidth, trim={0cm 0cm 0cm 0cm}]{./figures/#3}
    \end{figure}
}
\newcommand{\myFigureComparison}[4]{
    \begin{figure}[htpb]
    \centering
    \caption{#1}
    \label{#2}
    \includegraphics[width=.2\paperwidth, trim={8.5cm 0cm 0.5cm 0cm}]{./figures/#3}
    \includegraphics[width=.2\paperwidth, trim={0.5cm 0cm 8.5cm 0cm}]{./figures/#4}
    \end{figure}
}
% END CUSTOM MACRO -------------------------------------------------------------------------


\begin{document}

\maketitle

%The assignment is split into two parts: you are asked to solve a regression problem, and answer some questions. 
%You can use all the books, material, and help you need. 
%Bear in mind that the questions you are asked are similar to those you may find in the final exam, and are related to very important and fundamental machine learning concepts. 
%As such, sooner or later you will need to learn them to pass the course. 
%We will give you some feedback afterwards.\\

\noindent \textbf{!! !!  Note that this file is just meant as a template for the report, in which we reported \textit{part of} the assignment text for convenience. You must always refer to the text in the README.md file as the assignment requirements  !! !!}.

%----------------------------------------------------------------------------------------
%	Tasks
%----------------------------------------------------------------------------------------

\section*{Tasks}

This section should contain a detailed description of how you solved the assignment, including all required statistical analyses of the models' performance and a comparison between the linear regression and the model of your choice. Limit the assignment to 8-10 pages and do not include any code in the report.

\subsection*{Task 1}
Use the family of models $f(\mathbf{x}, \boldsymbol{\theta}) = \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2 + \theta_3 \cdot \cos(x_1) + \theta_4 \cdot x_2 \cdot x_2 + \theta_5 \cdot \tanh(x_1)$ to fit the data.


%theta_0 + theta_1 * x_1 + theta_2 * x_2 + theta_3 * cos(x_1) + theta_4 * x_2 * x_2 + theta_5 * tanh(x_1)

\begin{itemize}
	\item [a.] Write in the report the formula of the model substituting parameters
	      $\theta_0, \ldots, \theta_5$
	      with the estimates you've found:
	      $$f(\mathbf{x}, \boldsymbol{\theta})
		      =  0
		      + 5.019 \cdot x_1
		      - 4.000 \cdot x_2
		      + 6.983 \cdot \cos(x_1)
		      + 1.997 \cdot x_2 \cdot x_2
		      - 0.088 \cdot \tanh(x_1)$$

	      The coeff params are estimated until the 3rd decimal place.

	\item [b.] Evaluate the test performance of your model using the mean squared error as performance measure.

	\item [c.] Implement Lasso Regression, what do you observe?
	      What can you infer about the given family of models?
\end{itemize}


\subsection*{Task 2}
Consider any family of non-linear models of your choice to address the above regression problem.
\begin{itemize}
	\item [a.] Evaluate the test performance of your model using the mean squared error as performance measure (same data as Task 1).
	\item [b.] Compare your model with the linear regression of Task 1. Which one is {statistically} better?
\end{itemize}

\subsection*{Task 3 (Bonus)}
In the \href{https://github.com/FatimaEzzedinee/ML-bachelor-course-assignments-sp24}{\textbf{GitHub repository of the course}}, you will find a trained Torch learn model that we built using the same dataset you are given (\textbf{data\_bonus}).
This \textbf{baseline} model is able to achieve a MSE of \textbf{0.013}, when evaluated on the test set.
You will get extra points if you provide a model of your choice whose test performance is \textbf{better} (i.e., the MSE is lower) than ours. Of course, you must also tell us why your model is performing better.

%----------------------------------------------------------------------------------------
%	Questions
%----------------------------------------------------------------------------------------
%\newpage
\section*{Questions}

\subsection*{Q1. Training versus Validation}

% \begin{figure}[htbp] %
% 	\centering
% 	\includegraphics[width=0.8\textwidth]{ex_train_val_test.png}
% \end{figure}
\myFigure{Training versus validation exercise image}
{fig:ex-q1-training-vs-validation}
{ex_train_val_test.png}

\begin{itemize}
	\item[Q1.1] What is the whole figure about?
	\item[A1.1] ~\\
	      The Figure \ref{fig:ex-q1-training-vs-validation} shows the bias-variance tradeoff
	      for a model within the context of the regression problem.
	      The bias gives errors due to erroneous assumptions in the learning procedure.
	      In contrast, the variance yields errors due to the sensitivity of the model
	      to the training set fluctuations i.e. noise in the training dataset.
	      The three error lines displayed in the plot correspond to:
	      \begin{itemize}
		      \item Observed training error, from training set
		            used to fit the model on training data
		      \item Observed validation error, from test set assessing performance
		            meaning how well the model generalizes to unseen data
		      \item Expected test error, from an ideal unbiased dataset
		            assessing performance on unseen data
		            meaning how well the model would ideally generalize to unseen data
	      \end{itemize}

	      Along the x-axis, model complexity is intended as more hidden layers or neurons in the model.
	      In this particular model we can see that the observed training error
	      is decreasing as the model complexity increases, this will lead to overfitting.
	      Along the y-axis, the lower the performance metric, the better the model
	      since it measures the error of the model.

	\item[Q1.2] Explain the behaviours of the curves in each of the three highlighted sections in the figure, namely (a), (b), and (c).
	\item[A1.2] ~\\
	      In Figure \ref{fig:ex-q1-training-vs-validation}:
	      \begin{itemize}
		      \item[a.] window is associated with the start of the training process,
		            when the model is \textit{underfitting} the data, has high bias and low variance.
		            The model is excessively simple to capture the underlying patterns from the dataset,
		            that is evident from the high observed training error,
		            and also the validation error is high as a consequence of the non-generalization of the model
		            with unseen data from the test set.
		            We talk about test set and not validation set
		            because there is no model selection process involved.
		            The model has high bias and low variance at start.
		            As the model complexity increases, the training error decreases
		            and the model starts to learn the patterns in the data.
		            However, the validation error decreases at the beginning
		            but after a certain point starts to increase.

		      \item[b.] window is associated with the optimal bias-variance tradeoff for a model.
		            The ideal optimal model complexity $\theta^o$
		            resulting from the expected test error and the observed training error
		            references the red dotted vertical line,
		            and the optimal model complexity point according to the validation set $\theta_m$
		            resulting from the observed training and validation error
		            references the blue dotted line.

		      \item[c.] window is associated with the erroneous assumptions in the learning procedure,
		            when the model is \textit{overfitting} the data, has low bias and high variance.
		            The model is excessively complex to capture the underlying patterns from the dataset,
		            that is evident from the low observed training error as it started to learn
		            from the fluctuations of the independent input variables i.e. their variance,
		            and also the validation error is high as a consequence of the non-generalization
		            of the model with unseen data from the test set due to the noise learnt.
		            As the model complexity increases, the training error decreases
		            and the model starts to learn the noise patterns in the data.
		            The more you continue once overfitting started, the worse the performance assessment
		            result will be for the model, as the training error and validation error diverge
		            in performace metrics.
	      \end{itemize}

	      \begin{itemize}
		      \item[Q1.2.a] Can you identify any signs of overfitting or underfitting in the plot?
		            If yes, explain which sections correspond to which concept.

		      \item[A1.2.a] ~\\
		            As already vastly explained in previous answer A1.2,
		            the signs of underfitting are evident in Figure \ref{fig:ex-q1-training-vs-validation}
		            (a.) at the start of the training process.

		            The signs of overfitting are evident in Figure \ref{fig:ex-q1-training-vs-validation}
		            (c.) after the optimal model complexity point $\theta^o$,
		            when the model starts to learn the noise patterns in the data.

		      \item[Q1.2.b] How can you determine the optimal complexity of the model based on the given plot?

		      \item[A1.2.b] ~\\
		            The optimal complexity of the model can be determined based on
		            Figure \ref{fig:ex-q1-training-vs-validation}
		            window (b.) of given plot: between underfitting and overfitting.
		            Among the two optimal model complexity points $\theta^o$ and $\theta_m$ delimited by their
		            dotted vertical lines.
		            Where both the ideal expected test error and the observed validation error are minimized.
	      \end{itemize}

	\item[Q1.3] Is there any evidence of high approximation risk? Why?
	      If yes, in which of the below subfigures?

	\item[A1.3] ~\\
	      The approximation risk is defined as $\overline{V}(\theta^o) - V_I$
	      where $\overline{V}(\theta^o)$ is the structural risk
	      and $V_I$ is the inherent risk.
	      It depends on how well $f(\hat{\theta}, x) \approx g(x)$.
	      It can be improved by choosing a more appropriate model family $f'(\hat{\theta}, x)$
	      reducing the risk.
	      Figure \ref{fig:ex-q1-training-vs-validation} shows the relationship between
	      model complexity and error in performance.
	      The evidence of high approximation risk is present in the subfigures (a.) and (c.).
	      They are associated with underfitting and overfitting respectively.
	      High approximation risk for underfitting is due to the model
	      being too simple to capture underlying patterns,
	      while for overfitting is the poor generalization behavior on unseen data.
	      The spot with minimum approximation risk is the optimal model complexity point $\theta^o$
	      where the data generation function $g(x)$ is best approximated.

	\item[Q1.4] Do you think that increasing the model complexity can bring the training error to zero?
	      And the structural risk?

	\item[A1.4] ~\\
	      Increasing the model complexity could maybe bring the training error close or even to zero.
	      From the analysis of Figure \ref{fig:ex-q1-training-vs-validation} (c.) we can see that
	      achieving 0 training error is not a good goal, as it would lead to overfitting,
	      where the model learns the noise patterns in the data.
	      It is not sufficient to increase model complexity to reduce the structural risk. \newline
	      The structural risk is defined as a sum of 3 terms: \newline
	      $\overline{V}(\hat{\theta}) = [\overline{V}(\hat{\theta}) - \overline{V}(\theta^o)]
		      + [\overline{V}(\theta^o) - V_I]
		      + V_I$

	      The structural risk represents the model performance in the optimal case
	      as it describes the generalization abitility of the model,
	      and shows the discrepancy through loss function $\mathcal{L}$.
	      Improving the structural risk means bringing each term to 0:

	      \begin{itemize}
		      \item[] $\overline{V}(\hat{\theta}) - \overline{V}(\theta^o)$
		            estimation risk can reach 0 if the model family chosen reach the optimal $\theta^o$
		            coefficient parameters.

		      \item[] $\overline{V}(\theta^o) - V_I$ approximation risk can reach 0
		            if model family $f(\theta, x)$ exactly approximates the data generating function $g(x)$.

		      \item[] $V_I$ inherent risk can reach 0 if no noise is present in the dataset.
		            Since uncertainty is inherently present in the every physicl sensor, data always has it.
	      \end{itemize}
	      In conclusion, increasing the model complexity could potentially bring the training error to 0
	      but it is not a good goal
	      and the structural risk is not reduced by purely increasing model complexity.

	\item[Q1.5] If the X axis represented the training iterations instead,
	      would you think that the training procedure that generated the figure used early stopping?
	      Explaib why. (\textbf{NB}: ignore the subfigures and the dashed vertical lines)

	\item[A1.5] ~\\
	      If early stopping was used, then the plot should have ended when it reached the optimal model
	      $\theta_m$ according to the validation set in window (b.),
	      before the validation error started to increase.
	      Since it reached way far in the training iterations in window (c.),
	      reaching overfitting of training data and learning noise from it,
	      we can state that the procedure generating Figure \ref{fig:ex-q1-training-vs-validation}
	      did not used early stopping regularization technique.
\end{itemize}

\subsection*{Q2. Linear Regression}
Comment and compare how the (a.) training error, (b.) test error and (c.) coefficients
would change in the following cases:
\begin{itemize}
	\item[Q2.1] $x_3 = x_1 + 0.2 \cdot x_2$.

	\item[A2.1] ~\\
	      When adding the regressor to the model family formula
	      \myex{
		      f(x, \theta) &= \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2 + \theta_3 \cdot x_3\\
		      &= \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2 + \theta_3 \cdot (x_1 + 0.2 \cdot x_2)\\
		      &= \theta_0 + (\theta_1 + \theta_3) \cdot x_1 + (\theta_2 + 0.2 \cdot \theta_3) \cdot x_2\\
		      &= \theta_0 + \theta_1' \cdot x_1 + \theta_2' \cdot x_2\\
	      }
	      The new regressor addition did not change the model family structure.
	      Th new feature predictor $x_3$ is a linear combination of $x_1, x_2$,
	      hence is redundant and increases the multicollinearity in the model.
	      Such property could lead to overfitting and make the model learn noise patterns.
	      In windows:
	      \begin{itemize}
		      \item[a.] the train error could either remain the same or decrease due to multicollinearity
		      \item[b.] as consequence of multicollinearity, thus overfitting, the test error could increase
		      \item[c.] since $\theta_3$ is linear dependent, it lead to multiple solutions
		            for $\theta_1, \theta_2$ and the coefficients could be unstable and not unique,
		            as a result of overfitting
	      \end{itemize}

	\item[Q2.2] $x_3 = x_1 ** 2$ (in Python ** is the "power" operator, so  3 ** 2 = 3 * 3 = 9).

	\item[A2.2] ~\\
	      When adding the regressor to the model family formula
	      \myex{
		      f(x, \theta) &= \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2 + \theta_3 \cdot x_3\\
		      &= \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2 + \theta_3 \cdot x_1^2\\
		      &= \theta_0 + (\theta_1 + \theta_3 \cdot x_1) \cdot x_1 + \theta_2 \cdot x_2\\
		      &= \theta_0 + \theta_3' \cdot x_1 + \theta_2 \cdot x_2\\
	      }
	      The new regressor addition added a non-linear combination of existing features.
	      The model complexity is higher and the model family structure has changed.
	      Assuming the relationship between $y$ and predictors is non-linear,
	      the new regressor could capture the non-linear patterns in the data making better predictions.
	      If the assumption does not hold, then Occam's razor principle applies,
	      and the new predictor could decrease overall performance.
	      In windows:
	      \begin{itemize}
		      \item[a.] the train error could decrease if the data is linear, otherwise it could increase
		      \item[b.] the test error could decrease if the data is linear, otherwise it could increase
		      \item[c.] the coefficients could be more stable and unique
		            as the model is able to capture the non-linear patterns.
		            This depends on regressor relevance, multicollinearity, the relationship linearity among
		            input independent variables $x_i$ and the dependent output variable $y$.
	      \end{itemize}

	\item[Q2.3] $x_3$ is a random variable independent from $y$.

	\item[A2.3] ~\\
	      When adding the regressor to the model family formula
	      \myex{
		      f(x, \theta) &= \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2 + \theta_3 \cdot x_3\\
	      }
	      Predictors should have a consequences on the dependent output variable $y$.
	      If that's not the case for $x_3$,
	      then it does not provide any additional information to the model
	      and it is not useful for the prediction,
	      bringing up noise problems and causing overfitting.
	      Overall, the model family structure change,
	      its complexity is higher,
	      and the new regressor could capture noise fluctuations making worse predictions.
	      \begin{itemize}
		      \item[a.] initially, the train error could decrease due to
		            multicollinearity and noise overfitting
		      \item[b.] the test could increase as a consequence of overfitting the training dataset
		            and not generalizing to unseen data
		      \item[c.] after regression, the independent random variable $x_3$
		            could either be close or set to 0
	      \end{itemize}

	\item[Q2.4] How would your answers change if you were using Lasso Regression?
	\item[A2.4] ~\\
	      \begin{itemize}
		      \item[Q2.1] since the main problem of $x_3$ is being a linear combination of
		            $x_1, x_2$ and the multicollinearity it brings
		            could be solved by Lasso regression
		            as the regularization mechanism could lower
		            or even set to 0 the coefficient parameter $\theta_3$ 
					or the couple $\theta_1, \theta_2$.
					So, it depends on the relevance of the predictor $x_3$ and the couple
					$x_1, x_2$ over the dependent output variable $y$. 

		      \item[Q2.2] as the model goes to a higher complexity order with $x_3 = x_1^2$,
		            the Lasso regression would generally decrease training error
		            and increase test error.
		            Since $x_3$ is a non-linear combination of $x_1$,
		            I would speculate that the coefficient parameter $\theta_3$
		            of $x_3$ not be set to 0 since $x_1$ is a relevant predictor.
		            The redundant nature of $x_3$ could probably lead to overfitting since it
		            amplifies $x_1$ fluctuation noise in the model learning process.

		      \item[Q2.3] the coefficient parameter $\theta_3$ of $x_3$ random variable input
		            would be set to 0 as Lasso regression purpose is to allow irrelevant predictors
		            to be set to 0.
		            The model will then be the same as the original one without $x_3$.
		            So the expected behavior on (a.), (b.) and (c.)
		            would be the same as in the original model with $\theta_3 = 0$.
	      \end{itemize}

	\item[Q2.5] Explain the motivation behind Ridge and Lasso regression and their principal differences.
	\item[A2.5] ~\\
\end{itemize}

\subsection*{Q3. Logistic Regression}
\begin{itemize}
	\item[Q3.1] What are the main differences between the logistic-regression and the perceptron?
	\item[A3.1] ~\\

	\item[Q3.2] Discuss the major limit they share and how neural networks can solve it.
	\item[A3.2] ~\\

	\item[Q3.3] What is the role of activation functions in feedforward neural networks.
	\item[A3.3] ~\\
\end{itemize}

\subsection*{
	Q4. Consider the regression problem shown in the picture \ref{fig:ex-q4-regression}
	below and answer each point.
}

% \begin{figure}[htbp] %
% 	\centering
% 	\includegraphics[width=0.7\textwidth]{figures/parabolic.jpg}
% \end{figure}
\myFigure{Regression problem}{fig:ex-q4-regression}{parabolic.jpg}

\begin{itemize}
	\item[Q4.1] Do you think a model of the family f(x, theta) = $\theta_0 + \theta_1 * x_1 + \theta_2 * x_2$ is a good choice for such task? Why?

	\item[A4.1] The simple linear model $f(x, \theta)$ would not be a good fit for the data.
	      It is evident that the data does not follow a linear trend.
	      The linear model would lead to underfitting where the model is not able
	      to capture the underlying patterns.
	      The relationship between the inputs variables
	      and the dependent output variable seems to be non-linear,
	      describing a quadratic interaction on both $x_1, x_2$ resulting in a parabolic shape.
	      A linear relationship among input variables as in $f(x, \theta)$
	      would not capture the curvature form of the data.

	\item[Q4.2] Do you think using a feed-forward neural network would improve the results?

	\item[A4.2] Using an FFNN would be a good choice for this problem task.
	      It is for sure better than the simple model with linear relationship
	      between independent input variable $x_1, x_2$
	      and dependent output variable $f(x, \theta)$.
	      In general, with FFNNs we can leverage the \textit{Universal Approximation Theorem (1991)}
	      which states that an FFNN with a single hidden layer containing a finite number of neurons
	      and a linear ouput neuron can approximate any continuous function
	      on compact subsets of $\mathbb{R}^n$.
	      Moreover, FFNN are highly flexible and can approximate virtually any function,
	      so complex non-linear relationship patterns,
	      which are hard for linear and polynomial regression,
	      are handled through the encapsulated abstraction of the neurons in the hidden layer(s).
	      In this case, the parabolic shape of the data would be captured,
	      as FFNN are a common choice for regression tasks with non-linear data.

\end{itemize}

\end{document}
